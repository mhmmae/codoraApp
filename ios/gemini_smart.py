#!/usr/bin/env python3
import os
import sys
import json
import requests
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
sys.path.append(str(Path(__file__).parent))
try:
    from conversation_manager import ConversationManager
except ImportError:
    print("ØªØ­Ø°ÙŠØ±: Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ conversation_manager")
    ConversationManager = None

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
try:
    from error_analyzer import SmartErrorAnalyzer, ErrorReportGenerator, analyze_errors, SolutionEngine
except ImportError:
    print("ØªØ­Ø°ÙŠØ±: Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ error_analyzer")
    SmartErrorAnalyzer = None

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
try:
    from natural_language_processor import (
        SmartTextProcessor, ContextualUnderstanding,
        SmartResponseGenerator, process_user_input
    )
except ImportError:
    print("ØªØ­Ø°ÙŠØ±: Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ natural_language_processor")
    SmartTextProcessor = None

class EnhancedGeminiCLI:
    def __init__(self):
        self.config_dir = Path.home() / '.gemini-enhanced'
        self.config_file = self.config_dir / 'config.json'
        self.memory_file = self.config_dir / 'memory.json'
        self.context_cache_file = self.config_dir / 'context_cache.json'
        self.current_conversation_file = self.config_dir / 'current_conversation.txt'
        self.error_history_file = self.config_dir / 'error_history.json'

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        self.config_dir.mkdir(exist_ok=True)

        self.config = self.load_config()
        self.memory = self.load_memory()
        self.context_cache = self.load_context_cache()
        self.error_history = self.load_error_history()

        # ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        self.conversation_manager = ConversationManager() if ConversationManager else None
        self.current_conversation_id = self.load_current_conversation()

        # ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆÙ…Ø­Ø±Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ„
        self.solution_engine = SolutionEngine() if SmartErrorAnalyzer else None
        self.error_analyzer = SmartErrorAnalyzer(self.solution_engine) if SmartErrorAnalyzer and self.solution_engine else None

        # ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        if SmartTextProcessor:
            self.text_processor = SmartTextProcessor()
            self.context_analyzer = ContextualUnderstanding()
            self.response_generator = SmartResponseGenerator()
        else:
            self.text_processor = None
            self.context_analyzer = None
            self.response_generator = None

    def load_config(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        default_config = {
            "api_key": os.getenv('GEMINI_API_KEY', ''),
            "model": "gemini-1.5-pro",
            "temperature": 0.7,
            "max_tokens": 2048,
            "system_instruction": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…Ø·ÙˆØ± Ø®Ø¨ÙŠØ±. ØªÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¬ÙŠØ¯Ø§Ù‹ ÙˆØªÙ‚Ø¯Ù… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ©.",
            "language": "ar",
            "enable_memory": True,
            "enable_context_cache": True,
            "context_window_size": 10
        }

        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
                default_config.update(config)

        return default_config

    def save_config(self):
        """Ø­ÙØ¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)

    def load_memory(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        if self.memory_file.exists():
            with open(self.memory_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []

    def save_memory(self):
        """Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 50 Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙ‚Ø·
        if len(self.memory) > 50:
            self.memory = self.memory[-50:]

        with open(self.memory_file, 'w', encoding='utf-8') as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)

    def load_context_cache(self):
        """ØªØ­Ù…ÙŠÙ„ context cache"""
        if self.context_cache_file.exists():
            with open(self.context_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_context_cache(self):
        """Ø­ÙØ¸ context cache"""
        with open(self.context_cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.context_cache, f, ensure_ascii=False, indent=2)

    def get_relevant_context(self, prompt):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"""
        if not self.config['enable_memory'] or not self.memory:
            return ""

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        relevant_conversations = []
        prompt_words = set(prompt.lower().split())

        for conv in self.memory[-self.config['context_window_size']:]:
            conv_words = set(conv['prompt'].lower().split())
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¨Ø³ÙŠØ·
            similarity = len(prompt_words & conv_words) / len(prompt_words | conv_words)
            if similarity > 0.1:  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                relevant_conversations.append(conv)

        if relevant_conversations:
            context = "Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
            for conv in relevant_conversations[-3:]:  # Ø¢Ø®Ø± 3 Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø©
                context += f"Ø³: {conv['prompt'][:100]}...\n"
                context += f"Ø¬: {conv['response'][:200]}...\n\n"
            return context

        return ""

    def enhance_prompt(self, prompt, context_files=None):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù€ prompt Ø¨Ø°ÙƒØ§Ø¡"""
        enhanced_prompt = prompt

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
        if self.text_processor and self.context_analyzer and self.response_generator:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ
            analysis = self.text_processor.analyze_text(prompt)

            # ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚
            context = self.context_analyzer.understand_context(prompt, analysis)

            # ØªÙˆÙ„ÙŠØ¯ prompt Ù…Ø­Ø³Ù‘Ù†
            enhanced_prompt = self.response_generator.generate_enhanced_prompt(
                prompt, analysis, context
            )

            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù€ prompt
            if analysis.entities.get('files'):
                enhanced_prompt += f"\n\nÙ…Ù„ÙØ§Øª Ù…Ø°ÙƒÙˆØ±Ø©: {', '.join(analysis.entities['files'])}"

            if analysis.entities.get('errors'):
                enhanced_prompt += f"\n\nØ£Ø®Ø·Ø§Ø¡ Ù…Ø°ÙƒÙˆØ±Ø©: {', '.join(analysis.entities['errors'])}"

        else:
            # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ù…ØªÙˆÙØ±Ø§Ù‹
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if self.config['enable_memory']:
                context = self.get_relevant_context(prompt)
                if context:
                    enhanced_prompt = f"{context}\nØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ: {prompt}"

        # Ø¥Ø¶Ø§ÙØ© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡Ø§
        if context_files:
            file_context = "\nÙ…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª:\n"
            for file_path in context_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()[:2000]  # Ø£ÙˆÙ„ 2000 Ø­Ø±Ù
                        file_context += f"\n--- {file_path} ---\n{content}\n"
                except Exception as e:
                    file_context += f"\n--- {file_path} ---\nØ®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}\n"

            enhanced_prompt = f"{file_context}\n\n{enhanced_prompt}"

        return enhanced_prompt

    def call_gemini_api(self, prompt):
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Gemini API"""
        if not self.config['api_key']:
            return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† GEMINI_API_KEY"

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.config['model']}:generateContent"

        headers = {
            'Content-Type': 'application/json',
        }

        data = {
            'contents': [
                {
                    'parts': [
                        {'text': prompt}
                    ]
                }
            ],
            'generationConfig': {
                'temperature': self.config['temperature'],
                'maxOutputTokens': self.config['max_tokens'],
            },
            'systemInstruction': {
                'parts': [
                    {'text': self.config['system_instruction']}
                ]
            }
        }

        try:
            response = requests.post(
                f"{url}?key={self.config['api_key']}",
                headers=headers,
                json=data
            )
            response.raise_for_status()

            result = response.json()
            return result['candidates'][0]['content']['parts'][0]['text']

        except requests.exceptions.RequestException as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {e}"
        except KeyError as e:
            return f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {e}"

    def generate(self, prompt, context_files=None, save_to_memory=True):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        enhanced_prompt = self.enhance_prompt(prompt, context_files)
        response = self.call_gemini_api(enhanced_prompt)

        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        if save_to_memory and self.config['enable_memory']:
            conversation = {
                'timestamp': datetime.now().isoformat(),
                'prompt': prompt,
                'response': response,
                'enhanced_prompt': enhanced_prompt != prompt
            }
            self.memory.append(conversation)
            self.save_memory()

        # Ø­ÙØ¸ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯
        if save_to_memory and self.conversation_manager:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
            if not self.current_conversation_id:
                self.current_conversation_id = self.conversation_manager.create_conversation()
                self.save_current_conversation(self.current_conversation_id)

            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            self.conversation_manager.add_message(
                self.current_conversation_id,
                prompt,
                response,
                {
                    'enhanced_prompt': enhanced_prompt != prompt,
                    'context_files': context_files is not None,
                    'files_count': len(context_files) if context_files else 0
                }
            )

        return response

    def analyze_code(self, file_paths, question):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯"""
        if not file_paths:
            return "Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù…Ù„ÙØ§Øª Ù„Ù„ØªØ­Ù„ÙŠÙ„"

        code_context = "\n=== ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ ===\n"

        for file_path in file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    code_context += f"\n--- {file_path} ---\n{content[:3000]}\n"
            except Exception as e:
                code_context += f"\n--- {file_path} ---\nØ®Ø·Ø£: {e}\n"

        enhanced_question = f"{code_context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nÙŠØ±Ø¬Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„."

        return self.call_gemini_api(enhanced_question)

    def configure(self, key, value):
        """ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        if key in self.config:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            if key in ['temperature']:
                value = float(value)
            elif key in ['max_tokens', 'context_window_size']:
                value = int(value)
            elif key in ['enable_memory', 'enable_context_cache']:
                value = value.lower() in ['true', '1', 'yes', 'on']

            self.config[key] = value
            self.save_config()
            return f"ØªÙ… ØªØ­Ø¯ÙŠØ« {key} Ø¥Ù„Ù‰ {value}"
        else:
            return f"Ø§Ù„Ù…ÙØªØ§Ø­ {key} ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"

    def show_config(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        print("Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:")
        for key, value in self.config.items():
            if key == 'api_key':
                print(f"  {key}: {'*' * len(str(value)) if value else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
            else:
                print(f"  {key}: {value}")

    def show_memory(self, limit=10):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        if not self.memory:
            print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ø­ÙÙˆØ¸Ø©")
            return

        print(f"Ø¢Ø®Ø± {min(limit, len(self.memory))} Ù…Ø­Ø§Ø¯Ø«Ø§Øª:")
        for i, conv in enumerate(self.memory[-limit:], 1):
            timestamp = datetime.fromisoformat(conv['timestamp']).strftime('%Y-%m-%d %H:%M')
            print(f"\n{i}. [{timestamp}]")
            print(f"   Ø³: {conv['prompt'][:100]}...")
            print(f"   Ø¬: {conv['response'][:200]}...")

    # ==== Ø¯ÙˆØ§Ù„ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ====

    def load_current_conversation(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        if self.current_conversation_file.exists():
            try:
                return self.current_conversation_file.read_text(encoding='utf-8').strip()
            except:
                return None
        return None

    def save_current_conversation(self, conversation_id):
        """Ø­ÙØ¸ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        if conversation_id:
            self.current_conversation_file.write_text(conversation_id, encoding='utf-8')
        elif self.current_conversation_file.exists():
            self.current_conversation_file.unlink()

    def new_conversation(self, title=None):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        if not title:
            title = f"Ù…Ø­Ø§Ø¯Ø«Ø© {datetime.now().strftime('%Y-%m-%d %H:%M')}"

        conversation_id = self.conversation_manager.create_conversation(title)
        self.current_conversation_id = conversation_id
        self.save_current_conversation(conversation_id)

        return f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©: {title} (ID: {conversation_id})"

    def list_conversations(self, limit=20, search_term=None):
        """Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        conversations = self.conversation_manager.list_conversations(limit, search_term)

        if not conversations:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ø­ÙÙˆØ¸Ø©"

        output = f"ğŸ“š Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© ({len(conversations)}):\n\n"

        for i, conv in enumerate(conversations, 1):
            current_marker = "ğŸ”¥" if conv['id'] == self.current_conversation_id else "  "

            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ®
            try:
                last_updated = datetime.fromisoformat(conv['last_updated'])
                time_str = last_updated.strftime('%Y-%m-%d %H:%M')
            except:
                time_str = conv['last_updated'][:16]

            # Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª
            tags = " ".join([f"#{tag}" for tag in conv.get('tags', [])])
            tags_str = f" {tags}" if tags else ""

            output += f"{current_marker} {i:2d}. [{conv['id']}] {conv['title']}\n"
            output += f"     ğŸ“… {time_str} | ğŸ’¬ {conv['message_count']} Ø±Ø³Ø§Ù„Ø©{tags_str}\n\n"

        if self.current_conversation_id:
            output += f"Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©: {self.current_conversation_id}\n"

        return output

    def load_conversation(self, conversation_id):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø­Ø¯Ø¯Ø©"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        conversation = self.conversation_manager.get_conversation(conversation_id)
        if not conversation:
            return f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conversation_id}"

        self.current_conversation_id = conversation_id
        self.save_current_conversation(conversation_id)

        return f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conversation['title']} ({len(conversation['messages'])} Ø±Ø³Ø§Ù„Ø©)"

    def show_conversation(self, conversation_id=None, limit=10):
        """Ø¹Ø±Ø¶ Ù…Ø­ØªÙˆÙ‰ Ù…Ø­Ø§Ø¯Ø«Ø©"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        conv_id = conversation_id or self.current_conversation_id
        if not conv_id:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø­Ø¯Ø¯Ø©"

        conversation = self.conversation_manager.get_conversation(conv_id)
        if not conversation:
            return f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conv_id}"

        output = f"ğŸ“– Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conversation['title']}\n"
        output += f"ğŸ†” ID: {conversation['id']}\n"
        output += f"ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: {conversation['created_at'][:19]}\n"
        output += f"ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„: {len(conversation['messages'])}\n"

        if conversation.get('tags'):
            output += f"ğŸ·ï¸  Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª: {', '.join(conversation['tags'])}\n"

        output += "=" * 60 + "\n\n"

        # Ø¹Ø±Ø¶ Ø¢Ø®Ø± Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        messages = conversation['messages'][-limit:]
        for i, msg in enumerate(messages, 1):
            timestamp = datetime.fromisoformat(msg['timestamp']).strftime('%Y-%m-%d %H:%M')
            output += f"[{timestamp}] Ø§Ù„Ø±Ø³Ø§Ù„Ø© {i}:\n"
            output += f"ğŸ‘¤ Ø§Ù„Ø³Ø¤Ø§Ù„: {msg['prompt']}\n"
            output += f"ğŸ¤– Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {msg['response'][:500]}{'...' if len(msg['response']) > 500 else ''}\n"
            output += "-" * 40 + "\n\n"

        return output

    def delete_conversation(self, conversation_id):
        """Ø­Ø°Ù Ù…Ø­Ø§Ø¯Ø«Ø©"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        if conversation_id == self.current_conversation_id:
            self.current_conversation_id = None
            self.save_current_conversation(None)

        success = self.conversation_manager.delete_conversation(conversation_id)

        if success:
            return f"ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conversation_id}"
        else:
            return f"ÙØ´Ù„ ÙÙŠ Ø­Ø°Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conversation_id}"

    def search_conversations(self, search_term):
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        conversations = self.conversation_manager.list_conversations(50, search_term)

        if not conversations:
            return f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø¨Ø­Ø«: {search_term}"

        output = f"ğŸ” Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† '{search_term}' ({len(conversations)} Ù†ØªÙŠØ¬Ø©):\n\n"

        for i, conv in enumerate(conversations, 1):
            time_str = conv['last_updated'][:16]
            output += f"{i:2d}. [{conv['id']}] {conv['title']}\n"
            output += f"     ğŸ“… {time_str} | ğŸ’¬ {conv['message_count']} Ø±Ø³Ø§Ù„Ø©\n\n"

        return output

    def export_conversation(self, conversation_id, format='txt'):
        """ØªØµØ¯ÙŠØ± Ù…Ø­Ø§Ø¯Ø«Ø©"""
        if not self.conversation_manager:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±"

        content = self.conversation_manager.export_conversation(conversation_id, format)

        if content:
            filename = f"conversation_{conversation_id}.{format}"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            return f"ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ù„Ù‰: {filename}"
        else:
            return f"ÙØ´Ù„ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {conversation_id}"

    def analyze_text(self, text):
        """ØªØ­Ù„ÙŠÙ„ ÙˆÙÙ‡Ù… Ø§Ù„Ù†Øµ"""
        if not self.text_processor:
            return "Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØºÙŠØ± Ù…ØªÙˆÙØ±"

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ
        analysis = self.text_processor.analyze_text(text)

        # ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚
        context = self.context_analyzer.understand_context(text, analysis) if self.context_analyzer else {}

        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„
        output = f"ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ:\n"
        output += f"{'='*60}\n\n"

        output += f"ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {text[:100]}{'...' if len(text) > 100 else ''}\n\n"

        output += f"ğŸŒ Ø§Ù„Ù„ØºØ©: {analysis.language}\n"
        output += f"ğŸ’­ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {analysis.sentiment}\n"
        output += f"ğŸ“ˆ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {analysis.complexity:.2f}\n\n"

        output += f"ğŸ¯ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:\n"
        for intent in analysis.intents:
            output += f"   - {intent.type} (Ø«Ù‚Ø©: {intent.confidence:.2f})\n"

        output += f"\nğŸ·ï¸  Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {', '.join(analysis.keywords)}\n"
        output += f"ğŸ“š Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹: {', '.join(analysis.topics)}\n\n"

        output += f"ğŸ“¦ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:\n"
        for entity_type, entities in analysis.entities.items():
            if entities:
                output += f"   - {entity_type}: {', '.join(entities[:5])}\n"

        if context.get('suggested_responses'):
            output += f"\nğŸ’¡ Ø±Ø¯ÙˆØ¯ Ù…Ù‚ØªØ±Ø­Ø©:\n"
            for i, suggestion in enumerate(context['suggested_responses'], 1):
                output += f"   {i}. {suggestion}\n"

        if context.get('user_intent_pattern'):
            patterns = context['user_intent_pattern']
            output += f"\nğŸ‘¤ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:\n"
            output += f"   - Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø©: {patterns.get('preferred_language', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}\n"
            output += f"   - Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‚Ù†ÙŠ: {patterns.get('technical_level', 'Ù…ØªÙˆØ³Ø·')}\n"
            output += f"   - Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªÙØ§Ø¹Ù„: {patterns.get('interaction_style', 'Ø¹Ø§Ù…')}\n"

        return output

    def smart_generate(self, prompt, **kwargs):
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø¨ÙÙ‡Ù… Ø°ÙƒÙŠ Ù„Ù„Ù†Øµ"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹
        if self.text_processor:
            analysis_result = process_user_input(prompt, self)

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ prompt Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
            enhanced_prompt = analysis_result['enhanced_prompt']

            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚
            metadata = {
                'original_prompt': prompt,
                'language': analysis_result['analysis'].language,
                'intents': [i.type for i in analysis_result['analysis'].intents],
                'topics': analysis_result['analysis'].topics,
                'complexity': analysis_result['analysis'].complexity
            }

            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
            response = self.generate(enhanced_prompt, **kwargs)

            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù„Ù„Ø±Ø¯
            return {
                'response': response,
                'analysis': analysis_result,
                'metadata': metadata
            }
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
            return self.generate(prompt, **kwargs)

    def load_error_history(self):
        """ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        if self.error_history_file.exists():
            with open(self.error_history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []

    def save_error_history(self):
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 100 Ø®Ø·Ø£
        if len(self.error_history) > 100:
            self.error_history = self.error_history[-100:]

        with open(self.error_history_file, 'w', encoding='utf-8') as f:
            json.dump(self.error_history, f, ensure_ascii=False, indent=2)

    def analyze_errors_smart(self, file_paths, auto_fix=False, report_format='text'):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ"""
        if not self.error_analyzer:
            return "Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØºÙŠØ± Ù…ØªÙˆÙØ±. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ error_analyzer.py"

        all_errors = []
        fixed_count = 0

        for file_path in file_paths:
            if not Path(file_path).exists():
                print(f"âš ï¸  Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
                continue

            print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {file_path}")
            errors = self.error_analyzer.analyze_file(file_path)

            if errors:
                print(f"âŒ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(errors)} Ø®Ø·Ø£")
                all_errors.extend(errors)

                # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
                for error in errors:
                    error_record = {
                        'timestamp': datetime.now().isoformat(),
                        'file': file_path,
                        'type': error.type.value,
                        'severity': error.severity.name,
                        'message': error.message,
                        'fixed': False
                    }
                    self.error_history.append(error_record)

                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
                if auto_fix:
                    fixed = self._auto_fix_errors(file_path, errors)
                    fixed_count += fixed

            else:
                print(f"âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡!")

        # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„
        self.save_error_history()

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        if all_errors:
            report_generator = ErrorReportGenerator()
            report = report_generator.generate_report(all_errors, report_format)

            if auto_fix:
                report += f"\n\nğŸ”§ ØªÙ… Ø¥ØµÙ„Ø§Ø­ {fixed_count} Ø®Ø·Ø£ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"

            # Ø·Ù„Ø¨ ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù…Ù† Gemini
            if self.config.get('enable_ai_analysis', True):
                ai_analysis = self._get_ai_error_analysis(all_errors)
                if ai_analysis:
                    report += f"\n\nğŸ¤– ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:\n{ai_analysis}"

            return report
        else:
            return "âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø®Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡!"

    def _auto_fix_errors(self, file_path: str, errors: list) -> int:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        fixed_count = 0

        # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        original_content = content

        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØµÙ„Ø§Ø­ ÙƒÙ„ Ø®Ø·Ø£
        for error in errors:
            if error.solutions:
                # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø­Ù„ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
                for solution in error.solutions:
                    if solution.auto_applicable and solution.confidence > 0.8:
                        print(f"ğŸ”§ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù„: {solution.description}")

                        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù„
                        if solution.code_fix and error.line_number:
                            lines = content.split('\n')
                            if 0 <= error.line_number - 1 < len(lines):
                                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥ØµÙ„Ø§Ø­
                                # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹
                                fixed_count += 1
                                break

        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡
        if content != original_content:
            # Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            backup_path = f"{file_path}.backup"
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(original_content)

            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙØµÙ„Ø­
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙØµÙ„Ø­ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙÙŠ {backup_path})")

        return fixed_count

    def _get_ai_error_analysis(self, errors: list) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ Ù…Ù† Gemini"""
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        context = "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØªÙ‚Ø¯ÙŠÙ… Ù†ØµØ§Ø¦Ø­ Ù…ØªÙ‚Ø¯Ù…Ø©:\n\n"

        # ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        error_summary = {}
        for error in errors:
            error_type = error.type.value
            if error_type not in error_summary:
                error_summary[error_type] = []
            error_summary[error_type].append(error.message)

        for error_type, messages in error_summary.items():
            context += f"\n{error_type} ({len(messages)} Ø£Ø®Ø·Ø§Ø¡):\n"
            for msg in messages[:3]:  # Ø£ÙˆÙ„ 3 Ø£Ø®Ø·Ø§Ø¡ Ù…Ù† ÙƒÙ„ Ù†ÙˆØ¹
                context += f"  - {msg}\n"

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ø¤Ø§Ù„
        context += "\n\nÙŠØ±Ø¬Ù‰ ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØªÙ‚Ø¯ÙŠÙ…:\n"
        context += "1. Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø¬Ø°Ø±ÙŠ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n"
        context += "2. Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ù„ØªØ¬Ù†Ø¨Ù‡Ø§ Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹\n"
        context += "3. Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯\n"
        context += "4. Ø£ÙŠ Ø£Ù†Ù…Ø§Ø· Ù…Ù‚Ù„Ù‚Ø© ØªÙ„Ø§Ø­Ø¸Ù‡Ø§\n"

        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Gemini
        response = self.call_gemini_api(context)
        return response

    def suggest_fixes(self, error_description: str) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø­Ù„ÙˆÙ„ Ù„ÙˆØµÙ Ø®Ø·Ø£"""
        prompt = f"""Ø£Ù†Ø§ Ø£ÙˆØ§Ø¬Ù‡ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØªØ§Ù„ÙŠ:

{error_description}

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ…:
1. Ø´Ø±Ø­ Ù…ÙØµÙ„ Ù„Ù„Ø®Ø·Ø£
2. Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
3. Ø­Ù„ÙˆÙ„ Ù…Ù‚ØªØ±Ø­Ø© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©
4. Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ØµØ­ÙŠØ­
5. ÙƒÙŠÙÙŠØ© ØªØ¬Ù†Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø·Ø£ Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹

ÙŠØ±Ø¬Ù‰ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆÙ…ÙØµÙ„Ø©."""

        response = self.call_gemini_api(prompt)

        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
        self.error_history.append({
            'timestamp': datetime.now().isoformat(),
            'type': 'user_query',
            'description': error_description,
            'solution': response
        })
        self.save_error_history()

        return response

    def analyze_project_errors(self, project_path: str, file_extensions: list = None) -> str:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø®Ø·Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ ÙƒØ§Ù…Ù„"""
        if not self.error_analyzer:
            return "Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØºÙŠØ± Ù…ØªÙˆÙØ±"

        project_path = Path(project_path)
        if not project_path.exists():
            return f"Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {project_path}"

        # Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        if not file_extensions:
            file_extensions = ['.py', '.dart', '.js', '.ts', '.json', '.yaml', '.yml']

        print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {project_path}")

        all_errors = []
        files_analyzed = 0

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
        for ext in file_extensions:
            for file_path in project_path.rglob(f'*{ext}'):
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
                if any(part in file_path.parts for part in ['node_modules', '.git', '__pycache__', 'build', 'dist']):
                    continue

                files_analyzed += 1
                errors = self.error_analyzer.analyze_file(str(file_path))

                if errors:
                    print(f"âŒ {file_path.relative_to(project_path)}: {len(errors)} Ø£Ø®Ø·Ø§Ø¡")
                    all_errors.extend(errors)

        print(f"\nğŸ“Š ØªÙ… ØªØ­Ù„ÙŠÙ„ {files_analyzed} Ù…Ù„Ù")

        # ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„
        if all_errors:
            report = f"=== ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ===\n"
            report += f"Ø§Ù„Ù…Ø³Ø§Ø±: {project_path}\n"
            report += f"Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {files_analyzed}\n"
            report += f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {len(all_errors)}\n\n"

            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            error_stats = {}
            for error in all_errors:
                error_type = error.type.value
                if error_type not in error_stats:
                    error_stats[error_type] = 0
                error_stats[error_type] += 1

            report += "ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø·Ø§Ø¡:\n"
            for error_type, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True):
                report += f"  - {error_type}: {count} Ø®Ø·Ø£\n"

            # Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ù„ÙØ§Øª Ø£Ø®Ø·Ø§Ø¡Ù‹
            file_errors = {}
            for error in all_errors:
                if error.file_path:
                    if error.file_path not in file_errors:
                        file_errors[error.file_path] = 0
                    file_errors[error.file_path] += 1

            report += "\nğŸ”¥ Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ù„ÙØ§Øª Ø£Ø®Ø·Ø§Ø¡Ù‹:\n"
            for file_path, count in sorted(file_errors.items(), key=lambda x: x[1], reverse=True)[:5]:
                rel_path = Path(file_path).relative_to(project_path)
                report += f"  - {rel_path}: {count} Ø£Ø®Ø·Ø§Ø¡\n"

            # Ø§Ù„ØªÙˆØµÙŠØ§Øª
            report += "\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:\n"
            report += self._generate_project_recommendations(all_errors, error_stats)

            return report
        else:
            return f"âœ… Ù…Ù…ØªØ§Ø²! Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø®Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ({files_analyzed} Ù…Ù„Ù)"

    def _generate_project_recommendations(self, errors: list, stats: dict) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"""
        recommendations = []

        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        if stats.get('syntax', 0) > 5:
            recommendations.append("â€¢ ÙƒØ«Ø±Ø© Ø£Ø®Ø·Ø§Ø¡ Syntax - ÙŠÙÙ†ØµØ­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… IDE Ù…Ø¹ ÙØ­Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠ")

        if stats.get('import', 0) > 3:
            recommendations.append("â€¢ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ - ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª")

        if stats.get('security', 0) > 0:
            recommendations.append("â€¢ âš ï¸  ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ© - ÙŠØ¬Ø¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ÙÙˆØ±Ø§Ù‹")

        if stats.get('performance', 0) > 10:
            recommendations.append("â€¢ ÙØ±Øµ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ - Ø±Ø§Ø¬Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙØ§Ø¡Ø©")

        if not recommendations:
            recommendations.append("â€¢ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©ØŒ Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§ØªØ¨Ø§Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª")

        return '\n'.join(recommendations)

    def learn_from_fixes(self) -> str:
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"""
        if not self.error_history:
            return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³Ø¬Ù„ Ø£Ø®Ø·Ø§Ø¡ Ù„Ù„ØªØ¹Ù„Ù… Ù…Ù†Ù‡"

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        error_types = {}
        for error in self.error_history:
            error_type = error.get('type', 'unknown')
            if error_type not in error_types:
                error_types[error_type] = 0
            error_types[error_type] += 1

        report = "ğŸ“š Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n\n"
        report += f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø³Ø¬Ù„Ø©: {len(self.error_history)}\n\n"

        report += "ğŸ” Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹:\n"
        for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:5]:
            percentage = (count / len(self.error_history)) * 100
            report += f"  - {error_type}: {count} Ù…Ø±Ø© ({percentage:.1f}%)\n"

        # Ù†ØµØ§Ø¦Ø­ Ù…Ø®ØµØµØ©
        report += "\nğŸ’¡ Ù†ØµØ§Ø¦Ø­ Ù…Ø®ØµØµØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø³Ø¬Ù„Ùƒ:\n"

        if error_types.get('syntax', 0) > 5:
            report += "â€¢ Ù„Ø¯ÙŠÙƒ Ø£Ø®Ø·Ø§Ø¡ syntax Ù…ØªÙƒØ±Ø±Ø© - ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙƒÙˆØ¯ Ù‚Ø¨Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„\n"

        if error_types.get('import', 0) > 3:
            report += "â€¢ Ù…Ø´Ø§ÙƒÙ„ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ØªÙƒØ±Ø±Ø© - Ø£Ù†Ø´Ø¦ requirements.txt Ù„Ù„Ù…Ø´Ø±ÙˆØ¹\n"

        if error_types.get('undefined', 0) > 3:
            report += "â€¢ Ù…ØªØºÙŠØ±Ø§Øª ØºÙŠØ± Ù…Ø¹Ø±ÙØ© - Ø§Ø³ØªØ®Ø¯Ù… IDE Ù…Ø¹ Ø¥ÙƒÙ…Ø§Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠ\n"

        # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ÙØµÙ„Ø­Ø© Ù…Ù‚Ø§Ø¨Ù„ ØºÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø­Ø©
        fixed = len([e for e in self.error_history if e.get('fixed', False)])
        unfixed = len(self.error_history) - fixed

        report += f"\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:\n"
        report += f"  - Ø£Ø®Ø·Ø§Ø¡ ØªÙ… Ø¥ØµÙ„Ø§Ø­Ù‡Ø§: {fixed}\n"
        report += f"  - Ø£Ø®Ø·Ø§Ø¡ Ù„Ù… ØªÙØµÙ„Ø­: {unfixed}\n"

        if unfixed > fixed:
            report += "\nâš ï¸  Ù…Ø¹Ø¸Ù… Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù„Ù… ØªÙØµÙ„Ø­ - Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… --auto-fix"

        return report

def main():
    parser = argparse.ArgumentParser(description='Gemini CLI Ù…Ø­Ø³Ù†')
    parser.add_argument('prompt', nargs='*', help='Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø±Ø³Ø§Ù„Ù‡')
    parser.add_argument('--files', '-f', nargs='+', help='Ù…Ù„ÙØ§Øª Ù„Ù„Ø³ÙŠØ§Ù‚')
    parser.add_argument('--analyze', '-a', nargs='+', help='ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯')
    parser.add_argument('--config', '-c', nargs=2, metavar=('KEY', 'VALUE'), help='ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª')
    parser.add_argument('--show-config', action='store_true', help='Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª')
    parser.add_argument('--show-memory', type=int, nargs='?', const=10, help='Ø¹Ø±Ø¶ Ø§Ù„Ø°Ø§ÙƒØ±Ø©')
    parser.add_argument('--interactive', '-i', action='store_true', help='Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ')

    # Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    parser.add_argument('--list-conversations', '--lc', action='store_true', help='Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª')
    parser.add_argument('--new-conversation', '--new', type=str, nargs='?', const='', help='Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©')
    parser.add_argument('--load-conversation', '--load', type=str, help='ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø­Ø¯Ø¯Ø©')
    parser.add_argument('--show-conversation', '--show', type=str, nargs='?', const='', help='Ø¹Ø±Ø¶ Ù…Ø­ØªÙˆÙ‰ Ù…Ø­Ø§Ø¯Ø«Ø©')
    parser.add_argument('--delete-conversation', '--delete', type=str, help='Ø­Ø°Ù Ù…Ø­Ø§Ø¯Ø«Ø©')
    parser.add_argument('--search-conversations', '--search', type=str, help='Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª')
    parser.add_argument('--export-conversation', '--export', type=str, help='ØªØµØ¯ÙŠØ± Ù…Ø­Ø§Ø¯Ø«Ø©')

    # Ø£ÙˆØ§Ù…Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    parser.add_argument('--analyze-errors', '--ae', nargs='+', help='ØªØ­Ù„ÙŠÙ„ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø°ÙƒØ§Ø¡')
    parser.add_argument('--auto-fix', action='store_true', help='Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹')
    parser.add_argument('--analyze-project', '--ap', type=str, help='ØªØ­Ù„ÙŠÙ„ Ø£Ø®Ø·Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ ÙƒØ§Ù…Ù„')
    parser.add_argument('--suggest-fix', '--sf', type=str, help='Ø§Ù‚ØªØ±Ø§Ø­ Ø­Ù„ÙˆÙ„ Ù„ÙˆØµÙ Ø®Ø·Ø£')
    parser.add_argument('--learn-errors', action='store_true', help='Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©')
    parser.add_argument('--report-format', choices=['text', 'json', 'html'], default='text', help='ØµÙŠØºØ© ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡')

    # Ø£ÙˆØ§Ù…Ø± Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    parser.add_argument('--analyze-text', '--at', type=str, help='ØªØ­Ù„ÙŠÙ„ ÙˆÙÙ‡Ù… Ø§Ù„Ù†Øµ')
    parser.add_argument('--smart', '-s', action='store_true', help='Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù†Øµ')
    parser.add_argument('--understand', '-u', type=str, help='ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø¹Ù…Ù‚')

    args = parser.parse_args()

    cli = EnhancedGeminiCLI()

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙˆØ§Ù…Ø± Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
    if args.analyze_text:
        result = cli.analyze_text(args.analyze_text)
        print(result)
    elif args.understand:
        result = cli.analyze_text(args.understand)
        print(result)
        # ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø°ÙƒÙŠ Ø£ÙŠØ¶Ø§Ù‹
        print("\nğŸ¤– Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ:")
        smart_response = cli.smart_generate(args.understand)
        if isinstance(smart_response, dict):
            print(smart_response['response'])
        else:
            print(smart_response)
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙˆØ§Ù…Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    elif args.analyze_errors:
        result = cli.analyze_errors_smart(
            args.analyze_errors,
            auto_fix=args.auto_fix,
            report_format=args.report_format
        )
        print(result)
    elif args.analyze_project:
        result = cli.analyze_project_errors(args.analyze_project)
        print(result)
    elif args.suggest_fix:
        result = cli.suggest_fixes(args.suggest_fix)
        print(result)
    elif args.learn_errors:
        result = cli.learn_from_fixes()
        print(result)
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± - Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
    elif args.list_conversations:
        result = cli.list_conversations()
        print(result)
    elif args.new_conversation is not None:
        title = args.new_conversation if args.new_conversation else None
        result = cli.new_conversation(title)
        print(result)
    elif args.load_conversation:
        result = cli.load_conversation(args.load_conversation)
        print(result)
    elif args.show_conversation is not None:
        conv_id = args.show_conversation if args.show_conversation else None
        result = cli.show_conversation(conv_id)
        print(result)
    elif args.delete_conversation:
        result = cli.delete_conversation(args.delete_conversation)
        print(result)
    elif args.search_conversations:
        result = cli.search_conversations(args.search_conversations)
        print(result)
    elif args.export_conversation:
        result = cli.export_conversation(args.export_conversation)
        print(result)
    # Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    elif args.show_config:
        cli.show_config()
    elif args.show_memory is not None:
        cli.show_memory(args.show_memory)
    elif args.config:
        result = cli.configure(args.config[0], args.config[1])
        print(result)
    elif args.analyze:
        if not args.prompt:
            print("ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            return
        question = ' '.join(args.prompt)
        result = cli.analyze_code(args.analyze, question)
        print(result)
    elif args.interactive:
        print("Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ - Ø§ÙƒØªØ¨ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬")
        print("Ø£ÙˆØ§Ù…Ø± Ø¥Ø¶Ø§ÙÙŠØ©: 'conversations', 'new', 'load <id>', 'current'")
        print("Ø£ÙˆØ§Ù…Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: 'errors <file>', 'fix <error>', 'learn'")
        while True:
            try:
                prompt = input("\n> ")
                if prompt.lower() in ['exit', 'quit', 'Ø®Ø±ÙˆØ¬']:
                    break
                elif prompt.lower() in ['conversations', 'list']:
                    result = cli.list_conversations()
                    print(f"\n{result}")
                elif prompt.lower().startswith('new'):
                    parts = prompt.split(' ', 1)
                    title = parts[1] if len(parts) > 1 else None
                    result = cli.new_conversation(title)
                    print(f"\n{result}")
                elif prompt.lower().startswith('load '):
                    conv_id = prompt.split(' ', 1)[1]
                    result = cli.load_conversation(conv_id)
                    print(f"\n{result}")
                elif prompt.lower() in ['current', 'show']:
                    result = cli.show_conversation()
                    print(f"\n{result}")
                elif prompt.lower().startswith('search '):
                    search_term = prompt.split(' ', 1)[1]
                    result = cli.search_conversations(search_term)
                    print(f"\n{result}")
                elif prompt.lower().startswith('errors '):
                    file_path = prompt.split(' ', 1)[1]
                    result = cli.analyze_errors_smart([file_path])
                    print(f"\n{result}")
                elif prompt.lower().startswith('fix '):
                    error_desc = prompt.split(' ', 1)[1]
                    result = cli.suggest_fixes(error_desc)
                    print(f"\n{result}")
                elif prompt.lower() == 'learn':
                    result = cli.learn_from_fixes()
                    print(f"\n{result}")
                elif prompt.strip():
                    result = cli.generate(prompt)
                    print(f"\n{result}")
            except KeyboardInterrupt:
                print("\nØªÙ… Ø§Ù„Ø®Ø±ÙˆØ¬")
                break
    elif args.prompt:
        prompt = ' '.join(args.prompt)

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø°ÙƒÙŠ Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡
        if args.smart:
            result = cli.smart_generate(prompt, context_files=args.files)
            if isinstance(result, dict):
                print(result['response'])

                # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¥Ø°Ø§ Ø±ØºØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
                if '--verbose' in sys.argv:
                    print("\n" + "="*60)
                    print("ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
                    print(f"Ø§Ù„Ù„ØºØ©: {result['metadata']['language']}")
                    print(f"Ø§Ù„Ù†ÙˆØ§ÙŠØ§: {', '.join(result['metadata']['intents'])}")
                    print(f"Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹: {', '.join(result['metadata']['topics'])}")
                    print(f"Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {result['metadata']['complexity']:.2f}")
            else:
                print(result)
        else:
            result = cli.generate(prompt, args.files)
            print(result)
    else:
        parser.print_help()

if __name__ == '__main__':
    main()